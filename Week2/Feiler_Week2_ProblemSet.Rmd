---
title: "**BEE552 Biometry Week 2**"
author: "Maria Feiler"
date: "2/2/2022"
output:
output:
  html_document:
    css: doc.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
```

## My Learning Journey

Over the last week, I participated in Biometry in the following ways:

- I asked / answered **2** questions posed in class.

- I asked **2** questions in Slack.

- I answered **2** questions posed by other students on Slack.

- I came to Heather's office hours: **Yes**

- I came to Jose's office hours: **No**

- I met with Heather or Jose separately from office hours: **No**

Anything not falling into one of the above categories?

- 

On a scale of 1 (no knowledge) to 10 (complete expert), how would I rate
my comfort with R programming after this week? **5**

Any topics from last week that you are still confused about?

- 

\newpage

## Problem Set 

### Question 1

**Assume survival data (days from treatment to death) from two groups, a treatment group $(n_T=20)$ and a control/placebo group $(n_C=15)$ as follows:**

```{r treatment and control}
treatment = c(90, 91, 68, 90, 167, 26, 38, 89, 88, 99, 41, 123, 76, 88, 79, 96, 79, 122, 11, 23) 

control = c(52, 104, 146, 27, 46, 120, 5, 15, 11, 48, 30, 40, 8, 42, 74)
```

**a. Use a permutation method to test the null hypothesis that the treatment does not change survival time. Report the p-value associated with the two-tailed test of the null hypothesis. If the critical value $\alpha _{c}=0.05$, would you reject the null hypothesis? (Would the observed difference be considered significant [i.e. reject the null hypothesis] if we set $\alpha_{c}=0.01$? What about $\alpha_{c}=0.10$?)**

I will test the means of the two groups. Let $\overline{y}$ represent the mean of the treatment group and $\overline{z}$ represent the mean of the control group. Therefore $\hat{\theta} = \overline{y} - \overline{z}$. If the null is true, then $\hat{\theta} = 0$.

``` {r question 1, cache = TRUE}
# This code was sourced in part from the original poster's updated code: 
# https://stats.stackexchange.com/questions/176691/running-a-permutation-test-with-different-sample-sizes-in-r/176714

# Calculate the base statistic ybar - zbar
theta_hat <- mean(treatment) - mean(control)

# Create vector of zeros to compare the sample permutation test statistics against
dif <- integer(1000)

# Create vector of the differences between the sample permutation's means and the null (0)
for (i in 1:length(dif)) {
        samp <- sample(x = c(treatment, control),
                       replace = FALSE)

        samp_treat <- samp[1:20]
        samp_con <- samp[21:35]
        
        dif[i] <- mean(samp_treat) - mean(samp_con)
}
```

```{r 1 answer, echo = FALSE, comment = NA}
# Histogram of the differences
hist(dif, 
     xlim = c(-60, 60),
     ylim = c(0, 50),
     breaks = 100,
     main = expression(Distribution~of~hat(theta)*"*"),
     xlab = expression(hat(theta)*"*")
)

abline(v = theta_hat,
       col = "blue",
       lwd = 1.5
)

text(theta_hat + 2, 25, expression(hat(theta)), col = "blue")

# Get p-value using the absolute differences
p_value <- sum(abs(dif) > abs(theta_hat))/length(dif)

```

The quartiles of the permutations were `r paste(round(quantile(dif), digits = 2), "(", c("0%", "25%", "50%", "75%", "100%"), ")", sep = "") `.
The permutations' had an average `r paste(round(mean(dif), digits = 2), "±", round(sd(dif), digits = 2))` difference from the null hypothesis.  

The p-value of the original sample is `r p_value`. At $α_c=0.05$, the null hypothesis is `r ifelse(p_value > 0.05, "rejected", "accepted") `. At $α_c=0.01$, the null hypothesis is `r ifelse(p_value > 0.01, "rejected", "accepted") `. At $α_c=0.10$, the null hypothesis is `r ifelse(p_value > 0.1, "rejected", "accepted") `.  


**b. Given the context for the analysis, is 0.05 the best cut-off for significance? Why or why not? What might be an argument for using a different cut-off for significance?**

I argue that this 0.05 not a sufficient cutoff for this experiment because the sample sizes are so small. With a sample size of 35, that can accurately represent a population of only 350 patients, assuming that samples are at least 10% of the population based on standard practice. Assuming this drug is meant to be qidely circulated, this is not nearly a high enough sample size. If this sample size is all we have, then we need to make sure this treatment is EXTRA good. Expecially if this is a life saving/extending treatment. I would argue that $α_c=0.01$ is the best option of the three provided here.  

### Question 2

**Answer Question 1 using nonparametric bootstrap rather than permutation. Calculate confidence intervals for the difference in means between the two groups using bootstrap sampling. We can do this as follows:**

1. **Sample with replacement for Treatment and calculate the mean of that bootstrapped sample. Store that in a vector.**

2. **Sample with replacement for Control and calculate the mean of that bootstrapped sample. Store that in a vector.**

3. **Take the difference between these two means. Store that in a vector.**

4. **Start back at Step #1, and loop through some number of times.**

**The distribution of the difference of means is $\overline{Treatment} \space - \space \overline{Control} \space \sim \space N \left( E[X_{T}]-E[X_{C}] \space, \space \frac{Var[X_{T}]}{n_{T}}+\frac{Var[X_{C}]}{n_{C}} \right)$, where the standard deviation of $\overline{Treatment} \space - \space \overline{Control}$ should be $\sqrt{\frac{Var[X_{T}]}{n_{T}}+\frac{Var[X_{C}]}{n_{C}}}$.**

```{r boostrap test}


```

```{r bootstrap}
# Define number of iterations
nit <- 10

# Define the matrices into which the treatment boostrap and control bootstrap samples will be stored, and the vector in which the differences between the means will be stored.
bootT <- matrix(data = NA, 
                nrow = nit, 
                ncol = length(treatment),
                dimnames = list(c(1:nit),
                                c(1:length(treatment))
                                )
                )

bootC <- matrix(data = NA, 
                nrow = nit, 
                ncol = length(control),
                dimnames = list(c(1:nit),
                                c(1:length(control))
                                )
                )
bootDiff <- c()

# Run bootstrap for the number of iterations as defined by nit
for (i in 1:nit) {
    # Collect bootstrap samples for the treatment 
        bootT[i,] <- sample(x = treatment,
                            size = length(treatment),
                            replace = TRUE
                            )
        
    # Collect bootstrap samples for the control
        bootC[i,] <- sample(x = control,
                            size = length(control),
                            replace = TRUE
                            )
        
    # Collect the difference between the means of the treatment and control bootstrap samples 
        bootDiff[i] <- mean(bootT[[i]]) - mean(bootC[[i]])
}

```
  
  
**a. Using your bootstrap samples, what is the standard error of the mean for the Treatment group $(\hat{se}_{boot,T})$. How does this compare to $\sqrt{\frac{Var[X_{T}]}{n_{T}}}$? Approximately how many bootstrap samples are required to get a decent estimate (you define what constitutes “decent”)?**

```{r find.st.err function}
# Create function to find the standard error given a matrix of bootstrap samples and the number of repeats done in the bootstrap
find.st.err <- function(samp = NULL, repeats = NULL) {
    # Calculate the means of the bootstrap samples        
        means <- rowMeans(samp)
        
    # Calculate the mean of the bootstrap means
        theta <- sum(means)/repeats
        
    # Collect the sum of the squared bootstrap means minue theta
        temp <- c()
        for (i in 1:repeats) {temp[i] <- (means[i]-theta)^2}
        
    # Calculate the standard error
        sigma <- sqrt(sum(temp)/repeats)

}

```

```{r 2a answer}
# Find the standard error of the treatment bootstrap
sterr_treat <- find.st.err(bootT, nit)

# Find the standard deviation of the original treatment sample
sd_treat <- sqrt(var(treatment)/length(treatment))

```
  
  
The standard error of the treatment bootstrap (`r round(sterr_treat, digits = 2)`) is `r ifelse(sterr_treat > sd_treat, "greater than", "less than") ` the standard deviation of the original treatment sample (`r round(sd_treat, digits = 2)`).  
  
**b. Using your bootstrap samples, what is the standard error of the mean for the Control group $(\hat{se}_{boot,C})$). How does this compare to $\sqrt{\frac{Var[X_{C}]}{n_{C}}}$? Approximately how many bootstrap samples are required to get a decent estimate (you define what constitutes “decent”)?** 

```{r 2b answer}
# Find the standard error of the control bootstrap
sterr_con <- find.st.err(bootC, nit)

# Find the standard deviation of the original control sample
sd_con <- sqrt(var(control)/length(control))

```
  
The standard error of the control bootstrap (`r round(sterr_con, digits = 2)`) is `r ifelse(sterr_con > sd_con, "greater than", "less than") ` the standard deviation of the original control sample (`r round(sd_con, digits = 2)`).  
  

**c. Using $(\hat{se}_{boot,T})$ and $(\hat{se}_{boot,C})$, calculate the standard error of the difference in means $(\hat{se}_{boot,diff})$. Note that variances can be added and standard deviations are the square root of the variance.**  

```{r 2c answer}
# Calculate the standard error of the difference in means
sterr_diff <- sqrt(sterr_treat^2 + sterr_con^2)

```
  
The standard error of the difference in means is `r round(sterr_diff, digits = 2)`.  
  

**d. Use $\hat{se}_{boot,diff}$ to estimate the 95^th percentile confidence interval on the difference in means, assuming the interval is given by $(mean \space - \space 1.96\hat{se}_{boot,diff} \space , \space mean \space + \space 1.96\hat{se}_{boot,diff})$.**  

```{r 2d answer}


```
  

**e. We're now going to answer for 2d in a slightly different way using the percentiles of the bootstrapped samples. Calculate a 95th percentile confidence interval for the difference in mean survival days $\overline{Treatment}-\overline{Control}$ based on the quantiles of the bootstrapped samples.**  

```{r 2e answer}


```

