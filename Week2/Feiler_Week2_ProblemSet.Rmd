---
title: "BEE552 Biometry Week 2"
author: "Maria Feiler"
date: "2/2/2022"
output:
  html_document:
    css: w2psdoc.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
```

## My Learning Journey

Over the last week, I participated in Biometry in the following ways:

- I asked / answered **2** questions posed in class.

- I asked **2** questions in Slack.

- I answered **2** questions posed by other students on Slack.

- I came to Heather's office hours: **Yes**

- I came to Jose's office hours: **Yes**

- I met with Heather or Jose separately from office hours: **No**

Anything not falling into one of the above categories?

- 

On a scale of 1 (no knowledge) to 10 (complete expert), how would I rate
my comfort with R programming after this week? **5**

Any topics from last week that you are still confused about?

- I tried to do a nested for loop to automate the bootstrap tests, and for the life in me I couldn't figure it out. How would you do it? 

\newpage

## Problem Set 

### Question 1

*Assume survival data (days from treatment to death) from two groups, a treatment group $(n_T=20)$ and a control/placebo group $(n_C=15)$ as follows:*

```{r treatment and control}
treatment = c(90, 91, 68, 90, 167, 26, 38, 89, 88, 99, 41, 123, 76, 88, 79, 96, 79, 122, 11, 23) 

control = c(52, 104, 146, 27, 46, 120, 5, 15, 11, 48, 30, 40, 8, 42, 74)
```

*a. Use a permutation method to test the null hypothesis that the treatment does not change survival time. Report the p-value associated with the two-tailed test of the null hypothesis. If the critical value $\alpha _{c}=0.05$, would you reject the null hypothesis? (Would the observed difference be considered significant [i.e. reject the null hypothesis] if we set $\alpha_{c}=0.01$? What about $\alpha_{c}=0.10$?)*

<p class="ans">
I will test the means of the two groups. Let $\overline{y}$ represent the mean of the treatment group and $\overline{z}$ represent the mean of the control group. Therefore $\hat{\theta} = \overline{y} - \overline{z}$. If the null is true, then $\hat{\theta} = 0$.
</p>

``` {r question 1, cache = TRUE}
# This code was sourced in part from the original poster's updated code: 
# https://stats.stackexchange.com/questions/176691/running-a-permutation-test-with-different-sample-sizes-in-r/176714

# Calculate the base statistic ybar - zbar
theta_hat <- mean(treatment) - mean(control)

# Create vector of zeros to compare the sample permutation test statistics 
dif <- c()

# Create vector of the differences between the sample permutation's means and the null (0)
for (i in 1:1000) {
        samp <- sample(x = c(treatment, control),
                       replace = FALSE)

        samp_treat <- samp[1:20]
        samp_con <- samp[21:35]
        
        dif[i] <- mean(samp_treat) - mean(samp_con)
}
```

```{r 1 answer, echo = FALSE}
# Histogram of the differences
hist(dif, 
     xlim = c(-60, 60),
     ylim = c(0, 50),
     breaks = 100,
     main = expression(Distribution~of~hat(theta)*"*"),
     xlab = expression(hat(theta)*"*")
     )

abline(v = theta_hat,
       col = "blue",
       lwd = 1.5
       )

text(x = theta_hat + 2, 
     y = 25, 
     labels = expression(hat(theta)), 
     col = "blue"
     )

# Get p-value using the absolute differences
p_value <- sum(abs(dif) > abs(theta_hat))/length(dif)

```

<p class="ans">
The quartiles of the permutations were `r paste(round(quantile(dif), digits = 2), "(", c("0%", "25%", "50%", "75%", "100%"), ")", sep = "") `.
The permutations had an average `r paste(round(mean(dif), digits = 2), "±", round(sd(dif), digits = 2))` difference from the null hypothesis.
</p>

<p class="ans">
The p-value of the original sample is `r p_value`. At $α_c=0.05$, the null hypothesis is `r ifelse(p_value > 0.05, "rejected", "accepted") `. At $α_c=0.01$, the null hypothesis is `r ifelse(p_value > 0.01, "rejected", "accepted") `. At $α_c=0.10$, the null hypothesis is `r ifelse(p_value > 0.1, "rejected", "accepted") `.
</p>

*b. Given the context for the analysis, is 0.05 the best cut-off for significance? Why or why not? What might be an argument for using a different cut-off for significance?*

<p class="ans">
I argue that this 0.05 not a sufficient cutoff for this experiment because the sample sizes are so small. With a sample size of 35, that can accurately represent a population of only 350 patients, assuming that samples are at least 10% of the population based on standard practice. Assuming this drug is meant to be widely circulated, this is not nearly a high enough sample size. If this sample size is all we have, then we need to make sure this treatment is EXTRA good. Especially if this is a life saving/extending treatment. I would argue that $α_c=0.01$ is the best option of the three provided here.  
</p>

### Question 2  

*Answer Question 1 using nonparametric bootstrap rather than permutation. Calculate confidence intervals for the difference in means between the two groups using bootstrap sampling. The distribution of the difference of means is $\overline{Treatment} \space - \space \overline{Control} \space \sim \space N \left( E[X_{T}]-E[X_{C}] \space, \space \frac{Var[X_{T}]}{n_{T}}+\frac{Var[X_{C}]}{n_{C}} \right)$, where the standard deviation of $\overline{Treatment} \space - \space \overline{Control}$ should be $\sqrt{\frac{Var[X_{T}]}{n_{T}}+\frac{Var[X_{C}]}{n_{C}}}$.*

<p class="ans">
I tested my bootstrap method for 1000, 10000, 100000, and 1000000 iterations, which produced the following differences between the the standard deviation of $\overline{Treatment} \space - \space \overline{Control}$ and $\sqrt{\frac{Var[X_{T}]}{n_{T}}+\frac{Var[X_{C}]}{n_{C}}}$:
</p>

```{r bootstrap tests, echo = FALSE}
kable(x = cbind(format(c(1000, 10000, 100000, 1000000), scientific = FALSE), c(0.962, 0.483, 0.418, 0.431)),
      col.names = c("Iterations", "Differences")
      )
```

<p class="ans">  
Since there are diminishing returns after 100000 iterations, I will use that number of repeats for the bootstrap.
</p>

```{r bootstrap, cache = TRUE}
# Define number of iterations
nit <- 100000

# Define the matrices into which the treatment bootstrap and control bootstrap samples will be stored, and the vector in which the differences between the means will be stored.

bootT <- matrix(data = NA, 
                nrow = nit, 
                ncol = length(treatment),
                dimnames = list(c(1:nit),
                                c(1:length(treatment))
                                )
                )

bootC <- matrix(data = NA, 
                nrow = nit, 
                ncol = length(control),
                dimnames = list(c(1:nit),
                                c(1:length(control))
                                )
                )

bootDiff <- c()

# Run bootstrap for the number of iterations as defined by nit
for (i in 1:nit) {
    # Collect bootstrap samples for the treatment 
        bootT[i,] <- sample(x = treatment,
                            size = length(treatment),
                            replace = TRUE
                            )
        
    # Collect bootstrap samples for the control
        bootC[i,] <- sample(x = control,
                            size = length(control),
                            replace = TRUE
                            )
        
    # Collect the difference between the means of the treatment and control bootstrap samples 
        bootDiff[i] <- mean(bootT[i,]) - mean(bootC[i,])
}

```

```{r confirm bootstrap}
# Test that the standard deviation of the difference of the bootstrapped means is close to the squareroot of the difference in variances
bootDiffSD <- sd(rowMeans(bootT)-rowMeans(bootC))

coVar <- sqrt(var(treatment)/length(treatment)+var(control)/length(control))

```
  
<p class="ans"> 
For this bootstrap run, the difference  between the the standard deviation of $\overline{Treatment} \space - \space \overline{Control}$ and $\sqrt{\frac{Var[X_{T}]}{n_{T}}+\frac{Var[X_{C}]}{n_{C}}}$ is `r round(abs(bootDiffSD - coVar), digits = 2)`, which is acceptable.
</p>
  
*a. Using your bootstrap samples, what is the standard error of the mean for the Treatment group $(\hat{se}_{boot,T})$. How does this compare to $\sqrt{\frac{Var[X_{T}]}{n_{T}}}$? Approximately how many bootstrap samples are required to get a decent estimate (you define what constitutes "decent")?*

```{r 2a answer}
# Find the standard error of the treatment bootstrap
bootTStErr <- sd(rowMeans(bootT))

# Find the standard deviation of the original treatment sample
treatmentVar <- sqrt(var(treatment)/length(treatment))

```
  
<p class="ans">
The standard error of the treatment bootstrap (`r round(bootTStErr, digits = 2)`) is `r ifelse(bootTStErr > treatmentVar, "greater than", "less than") ` the standard deviation of the original treatment sample (`r round(treatmentVar, digits = 2)`).
</p>
  
*b. Using your bootstrap samples, what is the standard error of the mean for the Control group $(\hat{se}_{boot,C})$). How does this compare to $\sqrt{\frac{Var[X_{C}]}{n_{C}}}$? Approximately how many bootstrap samples are required to get a decent estimate (you define what constitutes "decent")?*

```{r 2b answer}
# Find the standard error of the control bootstrap
bootCStErr <- sd(rowMeans(bootC))

# Find the standard deviation of the original control sample
controlVar <- sqrt(var(control)/length(control))

```
  
<p class="ans">
The standard error of the control bootstrap (`r round(bootCStErr, digits = 2)`) is `r ifelse(bootCStErr > controlVar, "greater than", "less than") ` the standard deviation of the original control sample (`r round(controlVar, digits = 2)`).
</p>

*c. Using $(\hat{se}_{boot,T})$ and $(\hat{se}_{boot,C})$, calculate the standard error of the difference in means $(\hat{se}_{boot,diff})$. Note that variances can be added and standard deviations are the square root of the variance.* 
  
```{r 2c answer}
# Calculate the standard error of the difference in means
bootDiffStErr <- sqrt(bootTStErr^2 + bootCStErr^2)

```
  
<p class="ans">
The standard error of the difference in means is `r round(bootDiffStErr, digits = 2)`.
</p>

*d. Use $\hat{se}_{boot,diff}$ to estimate the 95<sup>th</sup> percentile confidence interval on the difference in means, assuming the interval is given by $(mean \space - \space 1.96\hat{se}_{boot,diff} \space , \space mean \space + \space 1.96\hat{se}_{boot,diff})$.* 

```{r 2d answer}
# Calculate the confidence interval using the standard error of the bootstrap differences and the mean of the bootstrap differences
upper2d <- mean(bootDiff)+(1.96*bootDiffStErr)
lower2d <- mean(bootDiff)-(1.96*bootDiffStErr)

```

<p class="ans">
The 95<sup>th</sup> percentile confidence interval for the difference in means is (`r paste(round(lower2d, digits = 2), ", ", round(upper2d, digits = 2), sep = "")`).
</p>

*e. We're now going to answer for 2d in a slightly different way using the percentiles of the bootstrapped samples. Calculate a 95<sup>th</sup> percentile confidence interval for the difference in mean survival days $\overline{Treatment}-\overline{Control}$ based on the quantiles of the bootstrapped samples.*  

```{r 2e answer}
# Calculate the confidence interval using the quantiles of the bootstrap mean differences
lower2e <- quantile(bootDiff, probs = 0.025)
upper2e <- quantile(bootDiff, probs = 0.975)

```

<p class="ans">
The 95<sup>th</sup> percentile confidence interval for the difference in means is (`r paste(round(lower2e, digits = 2), ", ", round(upper2e, digits = 2), sep = "")`).
</p>

### Question 3
*Briefly (??? a paragraph, with drawings as needed) explain the relationship between testing a hypothesis and estimating an effect size with a confidence interval. Can you use an effect size and its confidence interval to test a hypothesis? If so, how? Why might providing an effect size and its confidence interval be preferable to providing just a p-value?*

<p class="ans">
Need to ask Heather, check Isabelle Notes
</p>