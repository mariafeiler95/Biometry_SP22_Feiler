---
title: "BEE552 Biometry Week 3"
author: "Maria Feiler"
date: "2/9/2022"
output:
  html_document:
    css: w3psdoc.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(knitr)
library(MASS)
```

## My Learning Journey

*Over the last week, I participated in Biometry in the following ways:*

<p class = "ans">
- I asked / answered **4** questions posed in class.

- I asked **#** questions in Slack.

- I answered **#** questions posed by other students on Slack.

- I came to Heather's office hours: **Yes**

- I came to Jose's office hours: **Yes**

- I met with Heather or Jose separately from office hours: **No**
</p>

*Anything not falling into one of the above categories?*  

<p class = "ans">
**No**
</p>

*On a scale of 1 (no knowledge) to 10 (complete expert), how would I rate my* 
*comfort with R programming after this week?* 

<p class = "ans">
**5**
</p> 

*Any topics from last week that you are still confused about?*

<p class = "ans">

</p>

\newpage

## Problem Set 

### Question 1

*Let X be a discrete random variable whose pdf is described in the table given here:*
```{r question 1, echo = FALSE}
x <- c(-1, 0, 1)
fx <- c("1/8", "6/8", "1/8")

kable(cbind("x" = x, 
            "f(x)" = fx))
```

*Find the following:*

<p class = "ans">
a. $E[X] = x_1f(x_1) + x_2f(x_2) +x_3f(x_3) = -1(1/8) + 0(6/8) + 1(1/8) = 0$

b. $P(X = 0) = 6/8$

c. $P(X \le 1) = P(X = -1) + P(X = 0) + P(X = 1) = 1$

d. $F(1) = P(X \le 1) = 1$

e. $F^{-1}(7/8) =$
</p>

### Question 2

*X and Y are independent random variables. Write the following expressions in terms of E[X] and E[Y]*

<p class = "ans">
a.

b.
</p>

\newpage

### Question 3

*Use R to convince yourself of the Central Limit Theorem using draws from any distribution that is not the Normal distribution. Briefly (2-3 sentences) explain your process and provide your code as well.*

```{r question 3, comment = NA, out.width = "80%", fig.align = "center"}
# Define the test iterations of 1000 Bernoulli trials to be run
# Code only shows the first test, but replace n[#] with the desired value in
# nit to produce the second and third histograms.
nit <- c(10, 100, 1000, 10000)

# Matrix to catch n[#] iterations of 1000 coin flips
bern <- matrix(0, 
               nrow = nit[1], 
               ncol = 1000)

# Run the iterations
for (i in 1:nit[1]){
        bern[i,] <- rbinom(n = 1000,
                           size = 1,
                           prob = 0.5)
}

# Get the mean of each iteration
Sn <- rowMeans(bern)

# Fit the means to a normal distribution
fit <- fitdistr(Sn, "normal")

# Define x values to plot the pdf over
xVals <-  seq(from = min(Sn), 
               to = max(Sn), 
               by = 0.001
               )

# Make histogram
hist(x = Sn, 
     # To plot density so the pdf is scaled properly
     freq = FALSE,
     breaks = 30,
     main = "Mean of 1000 Bernoulli Trials, 10 Repeats"
     )

# Plot pdf
lines(xVals, dnorm(xVals, 
                   mean = fit$estimate[1],
                   sd = fit$estimate[2]
                   )
      )

```
```{r question 1 logik, echo = FALSE, comment = NA}
# Report the log likelihood of this pdf
cat(paste("The log likelihood of the normal distribution ",
          paste("N(",
                round(fit$estimate[1], digits = 3),
                ", ",
                round(fit$estimate[2], digits = 3),
                ") ",
                sep = ""
                ),
          "is ",
          round(fit$loglik),
          ".",
          sep = ""
          )
    )
```

```{r question 3 cont, echo = FALSE, comment = NA, cache = TRUE, out.width = "80%", fig.align = "center"}
# Second Histogram

# Matrix to catch n[#] iterations of 1000 coin flips
bern <- matrix(0, 
               nrow = nit[2], 
               ncol = 1000)

# Run the iterations
for (i in 1:nit[2]){
        bern[i,] <- rbinom(n = 1000,
                           size = 1,
                           prob = 0.5)
}

# Get the mean of each iteration
Sn <- rowMeans(bern)

# Fit the means to a normal distribution
fit <- fitdistr(Sn, "normal")

# Define x values to plot the pdf over
xVals <-  seq(from = min(Sn), 
               to = max(Sn), 
               by = 0.001
               )

# Make histogram
hist(x = Sn, 
     # To plot density so the pdf is scaled properly
     freq = FALSE,
     breaks = 30,
     main = "Mean of 1000 Bernoulli Trials, 100 Repeats"
     )

# Plot pdf
lines(xVals, dnorm(xVals, 
                   mean = fit$estimate[1],
                   sd = fit$estimate[2]
                   )
      )

# Report the log likelihood of this pdf
cat(paste("The log likelihood of the normal distribution ",
          paste("N(",
                round(fit$estimate[1], digits = 3),
                ", ",
                round(fit$estimate[2], digits = 3),
                ") ",
                sep = ""
                ),
          "is ",
          round(fit$loglik),
          ".",
          sep = ""
          )
    )


# Third Histogram

# Matrix to catch n[#] iterations of 1000 coin flips
bern <- matrix(0, 
               nrow = nit[3], 
               ncol = 1000)

# Run the iterations
for (i in 1:nit[3]){
        bern[i,] <- rbinom(n = 1000,
                           size = 1,
                           prob = 0.5)
}

# Get the mean of each iteration
Sn <- rowMeans(bern)

# Fit the means to a normal distribution
fit <- fitdistr(Sn, "normal")

# Define x values to plot the pdf over
xVals <-  seq(from = min(Sn), 
               to = max(Sn), 
               by = 0.001
               )

# Make histogram
hist(x = Sn, 
     # To plot density so the pdf is scaled properly
     freq = FALSE,
     breaks = 30,
     main = "Mean of 1000 Bernoulli Trials, 1000 Repeats"
     )

# Plot pdf
lines(xVals, dnorm(xVals, 
                   mean = fit$estimate[1],
                   sd = fit$estimate[2]
                   )
      )

# Report the log likelihood of this pdf
cat(paste("The log likelihood of the normal distribution ",
          paste("N(",
                round(fit$estimate[1], digits = 3),
                ", ",
                round(fit$estimate[2], digits = 3),
                ") ",
                sep = ""
                ),
          "is ",
          round(fit$loglik),
          ".",
          sep = ""
          )
    )


# Fourth Histogram

# Matrix to catch n[#] iterations of 1000 coin flips
bern <- matrix(0, 
               nrow = nit[4], 
               ncol = 1000)

# Run the iterations
for (i in 1:nit[4]){
        bern[i,] <- rbinom(n = 1000,
                           size = 1,
                           prob = 0.5)
}

# Get the mean of each iteration
Sn <- rowMeans(bern)

# Fit the means to a normal distribution
fit <- fitdistr(Sn, "normal")

# Define x values to plot the pdf over
xVals <-  seq(from = min(Sn), 
               to = max(Sn), 
               by = 0.001
               )

# Make histogram
hist(x = Sn, 
     # To plot density so the pdf is scaled properly
     freq = FALSE,
     breaks = 30,
     main = "Mean of 1000 Bernoulli Trials, 10000 Repeats"
     )

# Plot pdf
lines(xVals, dnorm(xVals, 
                   mean = fit$estimate[1],
                   sd = fit$estimate[2]
                   )
      )

# Report the log likelihood of this pdf
cat(paste("The log likelihood of the normal distribution ",
          paste("N(",
                round(fit$estimate[1], digits = 3),
                ", ",
                round(fit$estimate[2], digits = 3),
                ") ",
                sep = ""
                ),
          "is ",
          round(fit$loglik),
          ".",
          sep = ""
          )
    )
```

<p class = "ans">
After running these Bernoulli tests with more and more iterations, the log likelihood values show that the normal distribution fits better the more iterations used. Using repeated Bernoulli trials, which about are as far from Normal as you can get, to produce a normal distribution of means convinces me of the Central Limit Theorem.
</p>

\newpage

### Question 4

*Assume that leaf biomass (in grams) from the plant Salix arctica can be described as having the following probability density function:*

$$f(x) = \frac{2}{(x+1)^{3}} \space for \space x > 0$$

*a. Another way of saying that this pdf is restricted to $x>0$ is to say that there is no support for $x\le0$. Why would this be the case here?*

<p class = "ans">
If you are measuring a mass, you cannot have a "negative" mass. Mass is entirely additive and nonnegative in the same way that height is. Therefore, there is no support for any PDF at $x < 0$ because it is impossible to have a probability for negative mass, something that cannot exist. 
</p>

*b. Prove (using the two requirements for a valid pdf) that this is a valid pdf.*

**ASK**

*c. Manually (i.e., using calculus) calculate the probability that a leaf has biomass between 0 g and 3 g. In other words, find $P(0<X<3)$.*

\begin{align}
P(0 < x < 3) &= \int_{0}^{3}\left(\frac{2}{(x+1)^{3}}\right) \; dx \\
&= 2 \int_{0}^{3}\left(\frac{1}{u^3}\right) \; du \; where \; u = x+1 \\
&= 2 \int_{0}^{3}(u^{-3}) \; du \\
&= 2 \left(\frac{u^{-3+1}}{-2+1}\right) \\
&= \require{cancel} \cancel{2} \left(\frac{u^{-2}}{-\cancel{2}}\right) \\
&= -u^{-2} \\
&= \frac{-1}{(x+1)^{2}} \; solved \; over \; 0 \; to \; 3 \\
&= \frac{-1}{(3+1)^{2}}-\left(-\frac{1}{(0+1)^{2}}\right) \\
&= -1/16 +1 \\
&= 15/16 \\
&= 0.9375 \\
\end{align}

*d. Use the integrate function in R to confirm this result numerically.*

```{r question 4d, comment = NA}
integrate(f = function(x){2/(x+1)^3},
          lower = 0,
          upper = 3
          )
```

\newpage

### Question 5

*Bliss and R.A. Fisher (1953) examined female European red mite counts (Panonychus ulmi) on McIntosh apple trees [Malus domestica [McIntosh]). Counts of the mites on 150 leaves are shown here:*

```{r question 5, echo = FALSE}
kable(rbind("Mites per Leaf" = c(0, 1, 2, 3, 4, 5, 6, 7, 8), 
            "Leaves Observed" = c(70, 38, 17, 10, 9, 3, 2, 1, 0)
            )
      )
```

*a. What is the expected value E[X] of mites per leaf in this sample (show your work, either an R script or a calculation)?*

```{r question 5a, comment = NA}
# Define vectors of the mites per leaf and number of leaves observed
mites <- c(0, 1, 2, 3, 4, 5, 6, 7, 8)
leaves <- c(70, 38, 17, 10, 9, 3, 2, 1, 0)

# Make vector of the mites observed on each leaf
obs <- c()

for (i in 1:length(mites)) {
        temp <- rep(mites[i], leaves[i])
        obs <- c(obs, temp)
}
```
```{r question 5a cont, eval = FALSE}
# Expected value (E[X]) is the mean of all the observations
mean(obs)
```
```{r question 5a ans, echo = FALSE, comment = NA}
cat("lambda =", 
    round(mean(obs), 
          digits = 2)
    )

```

*b. Assume for the moment that Bliss and Fisher used a Poisson distribution to describe the number of mites per leaf X $(X \sim Pois(\lambda))$. What would be the most reasonable value for the Poisson parameter $\lambda$ and why? *

<p class = "ans">
Lambda ($\lambda$) for a Poisson distribution is defined as the mean of the observations. As the sample size increases, $\lambda$ will converge on a normal distribution, or $\displaystyle \lim_{x \to \infty}(X \sim Pois(\lambda)) \to N(\lambda, \lambda)$, and as a result $\lambda$ will converge on $E[X]$. Therefore, $\lambda = 1.15$.
</p>

*c. Make a barplot (the R function histogram should work, but as these are discrete variables, we would normally refer to this as a bar plot) of Bliss and Fisher's data. Is the expected value also the most common value? Why or why not?*

```{r question 5c, echo = FALSE, out.width = "80%", fig.align = "center"}
BlissFisher <- barplot(setNames(leaves, mites),
                       main = "Mites per Leaf (Bliss and Fisher 1953)",
                       xlab = "Mites per Leaf",
                       ylab = "Leaves Observed", 
                       ylim = c(0, 75)
                       )

text(BlissFisher, 
     leaves + 2, 
     paste("n =", leaves), 
     cex = 0.8
     )

```

<p class = "ans">
The expected value is not the most common value because the expected value is not an integer. Since this distribution is defined by discrete observations, the expected value 1.15 will not be represented by the data at all. If we were to round to the nearest integer (1), it still does not represent the most common value (0). This is because it is impossible to get a mean of 0 if there are any observations that are greater than zero.
</p>

*d. What is the standard deviation of the number of mites per leaf? What is the standard error of the mean (SEM) number of mites per leaf? In one sentence, describe the interpretation of the standard error of the mean?*

```{r question 5d, eval = FALSE}
# To calculate the standard deviation
sd(obs)

# To calculate the standard error of the mean
sd(obs)/sqrt(length(obs))
```

```{r question 5d ans, echo = FALSE, comment = NA}
cat("The standard deviation of the mites per leaf is ", 
    sd(obs), 
    ".", 
    sep = ""
    )

cat("The standard error of the mean mites per leaf is ", 
    sd(obs)/sqrt(length(obs)), 
    ".", 
    sep = ""
    )
```

*e. To gain better intuition for the meaning of the standard error of the mean, create 1000 new datasets by sampling with replacement from the original dataset. Use those 1000 simulated datasests to calculate the standard error of the mean. Because of sampling error, this will not be exactly the same as what you calculated in part d, but it should be close.*

```{r question 5e, cache = TRUE}
# Define number of iterations
nit = 1000

# Create matrix to catch the 1000 trials
bootObs <- matrix(data = NA, 
                  nrow = nit, 
                  ncol = length(obs),
                  dimnames = list(c(1:nit),
                                  c(1:length(obs))
                                  )
                )

# Create 1000 more datasets sampling with replacement
for (i in 1:nit) {
        bootObs[i,] <- sample(x = obs,
                              size = length(obs),
                              replace = TRUE
                              )
}
```
```{r question 5e cont, eval = FALSE}
# Calculate the standard error of the mean of the bootstrap datasets
sd(rowMeans(bootObs))

```

```{r question 5e ans, echo = FALSE, comment = NA}
cat("The standard error of the mean mites per leaf is ", 
    sd(rowMeans(bootObs)), 
    ".", 
    sep = ""
    )
```

\newpage

### Question 6

*We are going to get some practice using these statistical distributions in a biological context, and in the process gain some extra practice writing R code. Read Viswanathan et al. (2008) for the biological context of this problem, but don't worry too much about the mathematical details (you can skim over Section 4).*  

*This question is designed to get you writing some more sophisticated R scripts, with more emphasis on loops and logic, but there are some statistical and biological goals as well. Before getting to the actual question, I want to emphasize the following "take-home" messages:*

1. *There are many, many statistical distributions. We have learned only a small subset of all the statistical distributions. Long-tailed distributions (such as the L`evy, Cauchy, and Pareto) have many uses in ecology and evolution.*

2. *Simulations provide a straightforward way of studying complex stochastic phenomena.*

3. *Understanding 'pattern' goes a long way to understanding the underlying 'process'. Statistical distributions provide a way to quantify the pattern of your data. In many cases, only certain biological or ecological mechanisms can generate those patterns.*  

*Write a short script to simulate the movement of animals operating according to the movement rules described below. For simplicity, we will assume only one-dimensional motion along a line. Animals can move forward, or backwards, along a line. For each animal, simulate 500 individuals each moving for 100 time steps each.*

#### Animal 1
*This animal moves by "Brownian" motion. In each time step, the individual chooses a random direction (50% probability of moving forward or backwards [this is a Binomial process!]) and moves a distance given by (the absolute value of) $N(\mu=0,\sigma^2=1)$.  Cut and paste your script and a histogram of the final location for each of the 500 individuals. Fit a normal distribution to that distribution - what is $\hat{\mu}$ and $\hat{\sigma^2}$^? *


```{r animal 1, cache = TRUE}
# Define function that describes the movement of this animal over 100 steps
# Steps default value = 100 as defined by question, but other values can be
# substituted as needed
animal1 <- function(steps = 100){     
        # Vector of random distances, mean and sd as defined by N(mu = 0, sigma = 1)
        distance <- abs(rnorm(steps,
                              mean = 0,
                              sd = 1)
                        )
        
        # Vector to catch the true distances traveled (accounting for length and 
        # direction, where moving right is a positive movement and moving left 
        # is a negative movement)
        result <- c()

        # Loop to determine distance and direction of each step
        for (i in 1:steps){  
                # Conducting a Bernoulli trial to determine right (success)
                # or left (failure), then appending the distance traveled based
                # on the ith distance in the previously defined vector of
                # normally distributed random distances
                if(rbinom(n = 1, size = 1, prob = 0.5) == 1) {
                        result[i] <- distance[i]
                }
                else{
                        result[i] <- -distance[i]
                }
        }
        
        # Vector with distances the animal traveled
        return(result)
}

# Run the animal1() function for 500 animals
simAnimal1 <- matrix(data = 0,
                     nrow = 500,                    # Number of animals
                     ncol = 100,                    # Number of steps
                     dimnames = list(seq(1, 500),
                                     seq(1, 100)
                                     )
                     )

for (i in 1:nrow(simAnimal1)){
        simAnimal1[i,] <- animal1()
}

# Get the final location of each animal 
finalAnimal1 <- rowSums(simAnimal1)

# Fit normal distribution to the animal final locations
fit <- fitdistr(finalAnimal1, "normal")
```

```{r animal 1 ans, echo = FALSE, out.width = "80%", fig.align = "center"}
xVals <-  seq(from = min(finalAnimal1), 
               to = max(finalAnimal1), 
               by = 1
               )

hist(finalAnimal1,
     freq = FALSE,
     xlab = "Final Animal Location",
     main = "Final Location of 500 Animals Moving Under Brownian Motion",
     breaks = 20
     )

lines(xVals, dnorm(xVals, 
                   mean = fit$estimate[1],
                   sd = fit$estimate[2]
                   )
      )

mtext(paste("N(", 
            round(fit$estimate[1], digits = 3), 
            ", ",
            round(fit$estimate[2], digits = 3),
            ") ",
            sep = ""
            ),
      side = 3,
      adj = 1,
      cex = 1.2
      )

```

```{r, echo = FALSE, eval = FALSE}
animal1 <- function(steps = 100){     
        # Vector of random distances, mean and sd as defined by N(mu = 0, sigma = 1)
        distance <- abs(rnorm(steps,
                              mean = 0,
                              sd = 1)
                        )
        
        # List to catch direction, absolute distance traveled per step, and 
        # the true distances traveled (accounting for length and direction, 
        # where moving right is a positive movement and moving left is a 
        # negative movement)
        temp <- list(stepDir = c(),
                     stepAbsDist = c(),
                     stepDist = c()
                     )
        
        # Vector to catch the results
        result <- c()

        # Loop to determine distance and direction of each step
        for (i in 1:steps){  
                # Conducting a Bernoulli trial to determine right (success)
                # or left (failure), then appending the distance traveled based
                # on the ith distance in the previously defined vector of
                # normally distributed random distances
                if(rbinom(n = 1, size = 1, prob = 0.5) == 1) {
                        temp$stepDir[i] <- "R"
                        temp$stepAbsDist[i] <- distance[i]
                        temp$stepDist[i] <- distance[i]
                }
                else{
                        temp$stepDir[i] <- "L"
                        temp$stepAbsDist[i] <- distance[i]
                        temp$stepDist[i] <- -distance[i]
                }
        }
        
        # Store the results of the for loop
        result <- c(result, temp)
        
        # 2D list with direction and distance of the animal
        return(result)
}
```